{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3538,"sourceType":"datasetVersion","datasetId":792}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:15:46.918668Z","iopub.execute_input":"2025-04-04T17:15:46.919001Z","iopub.status.idle":"2025-04-04T17:15:48.086614Z","shell.execute_reply.started":"2025-04-04T17:15:46.918968Z","shell.execute_reply":"2025-04-04T17:15:48.085262Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/noshowappointments/KaggleV2-May-2016.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/noshowappointments/KaggleV2-May-2016.csv\")\n\n# Encode target variable\ndf['No-show'] = df['No-show'].map({'Yes': 1, 'No': 0})\n\n# Drop irrelevant features\ndf.drop(columns=['PatientId', 'AppointmentID', 'ScheduledDay', 'AppointmentDay'], inplace=True)\n\n# Encode categorical variables\nlabel_enc = LabelEncoder()\ndf['Gender'] = label_enc.fit_transform(df['Gender'])\ndf['Neighbourhood'] = label_enc.fit_transform(df['Neighbourhood'])\n\n# Features and Target\nX = df.drop('No-show', axis=1)\ny = df['No-show']\n\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:15:58.520360Z","iopub.execute_input":"2025-04-04T17:15:58.520803Z","iopub.status.idle":"2025-04-04T17:15:59.900936Z","shell.execute_reply.started":"2025-04-04T17:15:58.520766Z","shell.execute_reply":"2025-04-04T17:15:59.899892Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"pip install imbalanced-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:16:30.513318Z","iopub.execute_input":"2025-04-04T17:16:30.513680Z","iopub.status.idle":"2025-04-04T17:16:36.012589Z","shell.execute_reply.started":"2025-04-04T17:16:30.513655Z","shell.execute_reply":"2025-04-04T17:16:36.011459Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\nRequirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->imbalanced-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->imbalanced-learn) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->imbalanced-learn) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.3->imbalanced-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.3->imbalanced-learn) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Store sampled data\nsampling_methods = {}\n\n# 1. Random Oversampling\nros = RandomOverSampler(random_state=42)\nX_ros, y_ros = ros.fit_resample(X_train, y_train)\nsampling_methods['Random Oversampling'] = (X_ros, y_ros)\n\n# 2. Random Undersampling\nrus = RandomUnderSampler(random_state=42)\nX_rus, y_rus = rus.fit_resample(X_train, y_train)\nsampling_methods['Random Undersampling'] = (X_rus, y_rus)\n\n# 3. Tomek Links (Undersampling)\ntl = TomekLinks()\nX_tl, y_tl = tl.fit_resample(X_train, y_train)\nsampling_methods['Tomek Links'] = (X_tl, y_tl)\n\n# 4. SMOTE\nsmote = SMOTE(random_state=42)\nX_smote, y_smote = smote.fit_resample(X_train, y_train)\nsampling_methods['SMOTE'] = (X_smote, y_smote)\n\n# 5. Class Weights (No resampling)\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:17:04.077992Z","iopub.execute_input":"2025-04-04T17:17:04.078466Z","iopub.status.idle":"2025-04-04T17:17:06.220820Z","shell.execute_reply.started":"2025-04-04T17:17:04.078429Z","shell.execute_reply":"2025-04-04T17:17:06.219697Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\nresults = []\n\n# Model on original data\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\ny_proba = clf.predict_proba(X_test)[:, 1]\nresults.append(['Original', \n                accuracy_score(y_test, y_pred),\n                f1_score(y_test, y_pred),\n                roc_auc_score(y_test, y_proba)])\n\n# With Sampling Methods\nfor name, (X_samp, y_samp) in sampling_methods.items():\n    clf = DecisionTreeClassifier(random_state=42)\n    clf.fit(X_samp, y_samp)\n    y_pred = clf.predict(X_test)\n    y_proba = clf.predict_proba(X_test)[:, 1]\n    results.append([name, \n                    accuracy_score(y_test, y_pred),\n                    f1_score(y_test, y_pred),\n                    roc_auc_score(y_test, y_proba)])\n\n# With Class Weights\nclf = DecisionTreeClassifier(random_state=42, class_weight=class_weight_dict)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\ny_proba = clf.predict_proba(X_test)[:, 1]\nresults.append(['Class Weights', \n                accuracy_score(y_test, y_pred),\n                f1_score(y_test, y_pred),\n                roc_auc_score(y_test, y_proba)])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:17:25.416547Z","iopub.execute_input":"2025-04-04T17:17:25.417144Z","iopub.status.idle":"2025-04-04T17:17:26.939236Z","shell.execute_reply.started":"2025-04-04T17:17:25.417104Z","shell.execute_reply":"2025-04-04T17:17:26.938174Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"results_df = pd.DataFrame(results, columns=['Technique', 'Accuracy', 'F1 Score', 'AUC'])\nprint(results_df.sort_values(by='F1 Score', ascending=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:17:47.774283Z","iopub.execute_input":"2025-04-04T17:17:47.774650Z","iopub.status.idle":"2025-04-04T17:17:47.791362Z","shell.execute_reply.started":"2025-04-04T17:17:47.774627Z","shell.execute_reply":"2025-04-04T17:17:47.790033Z"}},"outputs":[{"name":"stdout","text":"              Technique  Accuracy  F1 Score       AUC\n5         Class Weights  0.620887  0.330225  0.567960\n2  Random Undersampling  0.583763  0.325547  0.561667\n1   Random Oversampling  0.629362  0.319038  0.562439\n4                 SMOTE  0.619440  0.308358  0.554837\n3           Tomek Links  0.759160  0.197871  0.568272\n0              Original  0.759221  0.197749  0.567772\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 📊 Performance Comparison of Sampling Techniques\n\n### 🔍 Key Observations:\n\n- **Class Weights** achieved the **best F1 Score (0.3302)** and a competitive AUC (0.5680), making it the most balanced approach for handling class imbalance.\n- **Random Undersampling (0.3255 F1 Score)** performed slightly worse but removes data, which may not be ideal.\n- **Random Oversampling (0.3190 F1 Score)** and **SMOTE (0.3084 F1 Score)** improved class balance but didn’t outperform Class Weights.\n- **Tomek Links** and the **Original dataset** had the highest accuracy (0.7592), but their **F1 Scores were the lowest (~0.198)**, showing they struggled with the minority class.\n- **Tomek Links had the highest AUC (0.5683)**, indicating better class separation but a weaker F1 Score.\n\n### 🏆 Best Sampling Technique:\n✅ **Class Weights** is the best approach as it **maximizes F1 Score while maintaining a good AUC**, making it the most effective for handling imbalance in this dataset.\n\n---\n\n### 📌 Recommendation:\n- If you prioritize **balanced performance** → Use **Class Weights**.\n- If you want to **improve AUC** but don’t mind lower F1 Score → Use **Tomek Links**.\n- If you prefer **more training data** rather than undersampling → Use **SMOTE or Random Oversampling**.\n\n---\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}